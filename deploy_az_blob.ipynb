{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548584cb-9baa-470f-92cb-766f48105fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: azure-storage-file-datalake in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fd47bbc7-0cc3-4835-962e-ab5623c1bc32/lib/python3.9/site-packages (12.11.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fd47bbc7-0cc3-4835-962e-ab5623c1bc32/lib/python3.9/site-packages (from azure-storage-file-datalake) (0.6.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.26.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fd47bbc7-0cc3-4835-962e-ab5623c1bc32/lib/python3.9/site-packages (from azure-storage-file-datalake) (1.26.4)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.16.0b1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fd47bbc7-0cc3-4835-962e-ab5623c1bc32/lib/python3.9/site-packages (from azure-storage-file-datalake) (12.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fd47bbc7-0cc3-4835-962e-ab5623c1bc32/lib/python3.9/site-packages (from azure-storage-file-datalake) (4.6.2)\n",
      "Requirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.26.0->azure-storage-file-datalake) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.26.0->azure-storage-file-datalake) (2.27.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.9/site-packages (from azure-storage-blob<13.0.0,>=12.16.0b1->azure-storage-file-datalake) (3.4.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.16.0b1->azure-storage-file-datalake) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.16.0b1->azure-storage-file-datalake) (2.21)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-file-datalake) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-file-datalake) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-file-datalake) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.26.0->azure-storage-file-datalake) (2021.10.8)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98799402-b28d-46e6-87c9-0c0c64fa415f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Shared/Common Utilities/helper_utils_Indrani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98b8b39-409c-4fb8-abc9-f1fd4f41c70b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f218e570-354b-4377-8587-6e812b637148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###Read Populations file from ContainerBlob\n",
    "file_location = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/ontario___extended_aggregated_wsi_dataset.csv\"\n",
    "WasteW_count_major = spark.read.options(header=True, inferSchema=True).csv(file_location)\n",
    "WasteW_count_major.createOrReplaceTempView(\"WasteW_count_major\")\n",
    "#sqlDF = spark.sql(\"SELECT count(*) FROM WasteW_count_major\")\n",
    "WasteW_refine = WasteW_count_major.select(\"siteID\",\"sewershedPop\")\n",
    "#display(WasteW_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b73880d-b000-4799-aacb-6b318d6cd905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WasteW_refine = WasteW_refine.select(\"siteID\",\"sewershedPop\").distinct()\n",
    "# display(WasteW_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb90e0ab-5413-4e45-99c1-0919380eb9eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8729796-9ade-4bfb-b48b-5f677166a7d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/LATEST_TREND_COMBINED-2023-05-30.csv\n"
     ]
    }
   ],
   "source": [
    "###read the LatestTrends combined for comparison\n",
    "#####creating path, pulling date(latest tuesday), appending to the existing wasbs@container\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "Pres = date.today()\n",
    "#datenow = datetime.utcnow().strftime(\"-%Y-%m-%a\")\n",
    "offset = (Pres.weekday() - 1) % 7\n",
    "Date_T = str(Pres - timedelta(days=offset))\n",
    "address = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/LATEST_TREND_COMBINED-\"\n",
    "format = \".csv\"\n",
    "path = address+Date_T+format\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b5bf8c-74ce-46b5-9da2-c62be383174c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File loaded'"
     ]
    }
   ],
   "source": [
    "#### Reading the Path for the Tuesday upload of wastewater\n",
    "\n",
    "filelocation = path\n",
    "try:\n",
    "    FileTuesdayWW = spark.read.format(\"csv\").options(header=True, inferSchema=False).load  (filelocation)\n",
    "#display(FileTuesdayWW.withColumn(\"filename\",input_file_name()))\n",
    "    display(\"File loaded\")\n",
    "except Exception as e:\n",
    "    print(\"No new file exists\")\n",
    "    #e.getMessage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a082f7-f2aa-489a-b45e-ac2b8e2097c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###read the Tuesday LatestTrends combined for comparison\n",
    "WasteW_latesttrends = spark.read.options(header=True, inferSchema=True).csv(filelocation)\n",
    "WasteW_latesttrends.createOrReplaceTempView(\"WasteW_latesttrends\")\n",
    "WasteW_latesttrends=WasteW_latesttrends.na.replace(\"NA\",\"Null\")\n",
    "wwlatesttrends = spark.sql(\"SELECT count(*) FROM WasteW_latesttrends\")\n",
    "wwlatesttrends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70922541-7a16-42e7-87f5-d4d9c12e8c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lit, when\n",
    "\n",
    "#### joining LATEST_TREND file and ontario___extended_aggregated_wsi_dataset.csv by SITEID & POPULATION('sewershedPop')\n",
    "combined_LATEST_TREND = WasteW_latesttrends.join(WasteW_refine,['siteID'],how='inner')\n",
    "#display(combined_LATEST_TREND)\n",
    "\n",
    "####calling the New PHUnames.csv files stored in dbfs:/FileStore/wastewater/PHUNAMes.csv\n",
    "# df_PHUnames = spark.read.format(\"csv\").option(\"header\",\"true\").load('dbfs:/FileStore/wastewater/PHUNames.csv')\n",
    "# combined_LATEST_TREND = combined_LATEST_TREND.join(df_PHUnames,['sys_PHU'],how='inner')\n",
    "# combined_LATEST_TREND = combined_LATEST_TREND.drop(combined_LATEST_TREND['sys_PHU'])   \n",
    "\n",
    "###ETL based on Increasing, Decreasing or NoChange\n",
    "combined_LATEST_TREND = combined_LATEST_TREND.withColumn(\"Status\", when((combined_LATEST_TREND.wwSlope>10) & \\\n",
    "                             (combined_LATEST_TREND.wwSignificance==\"Yes\"), \"Increasing\")\n",
    "                             .when((combined_LATEST_TREND.wwSlope<=-10) & \\\n",
    "                             (combined_LATEST_TREND.wwSignificance==\"Yes\"), \"Decreasing\")\n",
    "                             .when((combined_LATEST_TREND.wwSlope>-10) | \\\n",
    "                             (combined_LATEST_TREND.wwSignificance==\"No\"), \"Little/No Change\")\n",
    "                             .otherwise(\"No Trend Available\"))\n",
    "\n",
    "##write transformed file back to blobContainer@wastewater\n",
    "combined_LATEST_TREND\n",
    "file_location_output = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/output\"\n",
    "combined_LATEST_TREND.coalesce(1).write.mode(\"overwrite\").options(header=True).format(\"csv\").save(file_location_output)\n",
    "\n",
    "#####write Table to Public/ist_0504_cctdash_0000_uc\n",
    "wastewater_output = config[\"cct_db_schema\"] + \".wastewater_output\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {wastewater_output}\")\n",
    "combined_LATEST_TREND.write.saveAsTable(wastewater_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e258e5e-0b59-4bb2-a089-c3caecfcfde9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: True"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "#############rename part-00000* file to wastewater_output.csv\n",
    "\n",
    "targetname = ''\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "file_system = FileSystemClient.from_connection_string(ConnectionString,file_system_name=\"wastewater\")\n",
    "\n",
    "paths = file_system.get_paths()\n",
    "for path in paths:\n",
    "    #print(path.name + '\\n')\n",
    "    if path.name.startswith('output/part-0000'):\n",
    "        targetname = path.name\n",
    "        \n",
    "#print(targetname)\n",
    "fileloc_RenameFile = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/\"+targetname\n",
    "old_name = fileloc_RenameFile\n",
    "new_name = r\"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/wastewater_output.csv\"\n",
    "\n",
    "dbutils.fs.mv(old_name, new_name, True)\n",
    "\n",
    "PATH = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/output/\"\n",
    "for i in dbutils.fs.ls(PATH):\n",
    "    dbutils.fs.rm(i[0],True)\n",
    "\n",
    "output = 'output'\n",
    "PATH = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/\"+output\n",
    "dbutils.fs.rm(PATH,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0742fd-6bd5-445b-99b9-ec086234d8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, sum, avg, concat, col, lit\n",
    "from pyspark.sql import functions as B\n",
    "\n",
    "count_sewersheds = combined_LATEST_TREND.groupBy(\"sys_PHU\",\"sys_PHU_region\").agg(countDistinct(\"sys_sewershedName\"). \\\n",
    "                   alias(\"Total_sewersheds_count\"))\n",
    "count_sewersheds_N = combined_LATEST_TREND.withColumn(\"Status_N\", when((combined_LATEST_TREND.Status == 'Increasing'), \"1\"))\n",
    "_fil_count_sewersheds_N = count_sewersheds_N.select(\"sys_PHU\",\"Status_N\").filter(col(\"Status_N\").isNotNull())\n",
    "combined_count_sewersheds = count_sewersheds.join(_fil_count_sewersheds_N,['sys_PHU'],how='inner')\n",
    "fil_count_sewersheds_N = combined_count_sewersheds.groupBy(\"sys_PHU\").count()\n",
    "final_count_sewershedPHU_Region = count_sewersheds.join(fil_count_sewersheds_N,['sys_PHU'],how='inner').drop(\"Status_N\").distinct(). \\\n",
    "                                withColumnRenamed(\"count\", \"Status_Increasing\")\n",
    "\n",
    "final_count_sewershedPHU_Region = final_count_sewershedPHU_Region.withColumn(\"Region\", lit(\"Region\"))\n",
    "                                  \n",
    "# display(final_count_sewershedPHU_Region)\n",
    "\n",
    "final_count_sewershedPHU_Region = final_count_sewershedPHU_Region.withColumn('PHU_region', B.concat(B.col('sys_PHU_region'), B.lit(\" Region\")))\n",
    "# final_count_sewershedPHU_Region = final_count_sewershedPHU_Region.withColumn('Status_Increasing', B.concat(B.col('Status_Increasing'), B.lit(\" of\")))\n",
    "\n",
    "\n",
    "final_count_sewershedPHU_Region = final_count_sewershedPHU_Region.drop(final_count_sewershedPHU_Region['sys_PHU_region']) \\\n",
    "                                                                 .drop(final_count_sewershedPHU_Region['Region']) \n",
    "\n",
    "display(final_count_sewershedPHU_Region)    #sys_PHU_region, Total_sewersheds_count,Region, Sewershed\n",
    "\n",
    "##write transformed file - PHU_increase_report.csv back to blobContainer@wastewater\n",
    "file_location_output = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/output\"\n",
    "final_count_sewershedPHU_Region.coalesce(1).write.mode(\"overwrite\").options(header=True).format(\"csv\").save(file_location_output)\n",
    "\n",
    "#####write Table to Public/ist_0504_cctdash_0000_uc\n",
    "finalcountSW_PHU_Region = config[\"cct_db_schema\"] + \".PHU_increase_report\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {finalcountSW_PHU_Region}\")\n",
    "final_count_sewershedPHU_Region.write.saveAsTable(finalcountSW_PHU_Region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8247e3cd-2eef-48f1-8e7b-7e60bd07b520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reports uploaded to blob successfully\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "#############rename part-00000* file to PHU_increase_report.csv\n",
    "\n",
    "targetname = ''\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "file_system = FileSystemClient.from_connection_string(ConnectionString,file_system_name=\"wastewater\")\n",
    "\n",
    "paths = file_system.get_paths()\n",
    "for path in paths:\n",
    "    #print(path.name + '\\n')\n",
    "    if path.name.startswith('output/part-0000'):\n",
    "        targetname = path.name\n",
    "        \n",
    "#print(targetname)\n",
    "fileloc_RenameFile = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/\"+targetname\n",
    "old_name = fileloc_RenameFile\n",
    "new_name = r\"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/PHU_increase_report.csv\"\n",
    "\n",
    "dbutils.fs.mv(old_name, new_name, True)\n",
    "\n",
    "PATH = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/output/\"\n",
    "for i in dbutils.fs.ls(PATH):\n",
    "    dbutils.fs.rm(i[0],True)\n",
    "\n",
    "output = 'output'\n",
    "PATH = \"wasbs://wastewater@cctdash0000prd0504blob.blob.core.windows.net/\"+output\n",
    "dbutils.fs.rm(PATH,True)\n",
    "\n",
    "print(\"Reports uploaded to blob successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "_Wastewater_final-auto",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
